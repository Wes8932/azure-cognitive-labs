{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "# Batch anomaly detection with the Anomaly Detector API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Batch anomaly detection with the Anomaly Detector API\n",
        "\n",
        "The *Anomaly Detector* API enables you to monitor and detect abnormalities in your time series data with machine learning. The Anomaly Detector API adapts by automatically identifying and applying the best-fitting models to your data, regardless of industry, scenario, or data volume. Using your time series data, the API determines boundaries for anomaly detection, expected values, and which data points are anomalies.\n",
        "\n",
        "Anomaly Detector provides two APIs that detect anomalies automatically in time series with simple parameters, which require no machine learning background. It is designed for the scenarios of operational monitoring, business KPI monitoring, and IoT monitoring. Using Anomaly Detector APIs, you can infuse anomaly detection capabilities into your own platform and business process.\n",
        "\n",
        "\n",
        "## Use the Anomaly Detector Cognitive Service\n",
        "\n",
        "Microsoft Azure includes a number of *cognitive services* that encapsulate common AI functions, including some that can help you build anomaly detection solutions.\n",
        "\n",
        "The *Anomaly Detector* cognitive service provides an obvious starting point for having an out-of-the-box solution for detecting anomalies in Azure. It uses automatically selects the right algorithm for the varities of anomalies inculding seasonality, spikes, dips and trend changes.\n",
        "\n",
        "### Create a Cognitive Services Resource\n",
        "\n",
        "Let's start by creating a **Anomaly Detector** resource in your Azure subscription:\n",
        "\n",
        "1. In another browser tab, open the Azure portal at https://portal.azure.com, signing in with your Microsoft account.\n",
        "2. Click the **&#65291;Create a resource** button, search for *Anomaly Detector *, and create a **Cognitive Services** resource with the following settings:\n",
        "    - **Name**: *Enter a unique name*.\n",
        "    - **Subscription**: *Your Azure subscription*.\n",
        "    - **Location**: *Choose any available region*:\n",
        "    - **Pricing tier**: F0\n",
        "    - **Resource group**: *Create a resource group with a unique name*.\n",
        "    - Select the check box for *I confirm I have read and understood the notice below* as this service is still under preview. \n",
        "3. Wait for deployment to complete. Then go to your cognitive services resource, and on the **Overview** page, click the link to manage the keys for the service. You will need the endpoint and keys to connect to your cognitive services resource from client applications.\n",
        "\n",
        "### Get the Key and Endpoint for your Cognitive Services resource\n",
        "\n",
        "To use your cognitive services resource, client applications need its  endpoint and authentication key:\n",
        "\n",
        "1. In the Azure portal, on the **Keys and Endpoint** page for your cognitive service resource, copy the **Key1** for your resource and paste it in the code below, replacing **YOUR_COG_KEY**.\n",
        "2. Copy the **endpoint** for your resource and and paste it in the code below, replacing **YOUR_COG_ENDPOINT**.\n",
        "3. Run the code in the cell below by clicking its green <span style=\"color:green\">&#9655</span> button."
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "### Use this Jupyter notebook to start visualizing anomalies as a batch with the Anomaly Detector API in Python.\n",
        "\n",
        "This notebook shows you how to send a batch anomaly detection request, and vizualize the anomalies found throughout the example data set. The graph created at the end of this notebook will display the following:\n",
        "* Anomalies found throughout the data set, highlighted.\n",
        "* The expected values versus the values contained in the data set.\n",
        "* Anomaly detection boundaries \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To start sending requests to the Anomaly Detector API, paste your subscription key you received after creating Anomaly Detector resource. \n",
        "cog_key = 'YOUR_COG_KEY'\n",
        "\n",
        "# Use the endpoint your received from overview section of the Anomaly Detector resource you created\n",
        "# the endpoint is like https://westus2.api.cognitive.microsoft.com/, different by regions. \n",
        "# The code concats anomalydetector/v1.0/timeseries/entire/detect to the end point for the API calls\n",
        "cog_endpoint = 'YOUR_COG_ENDPOINT'\n",
        "cog_endpoint += 'anomalydetector/v1.0/timeseries/entire/detect'\n",
        "\n",
        "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2020-07-13T10:54:32.934284Z",
          "end_time": "2020-07-13T10:54:32.968823Z"
        }
      },
      "cell_type": "code",
      "source": [
        "# To start sending requests to the Anomaly Detector API, paste your subscription key you received after creating Anomaly Detector resource. \n",
        "subscription_key = 'fe41873c6f134d01a9ac223b1af02be7' \n",
        "\n",
        "# Use the endpoint your received from overview section of the Anomaly Detector resource you created\n",
        "# the endpoint is like https://westus2.api.cognitive.microsoft.com/, different by regions, you need to concat anomalydetector/v1.0/timeseries/entire/detect\n",
        "\n",
        "endpoint = 'https://anomaly-demo.cognitiveservices.azure.com/anomalydetector/v1.0/timeseries/entire/detect'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2020-07-13T10:54:34.66064Z",
          "end_time": "2020-07-13T10:54:34.775776Z"
        }
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from __future__ import print_function\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# Import library to display results\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2020-07-13T10:54:36.232095Z",
          "end_time": "2020-07-13T10:54:36.3283Z"
        }
      },
      "cell_type": "code",
      "source": [
        "from bokeh.plotting import figure,output_notebook, show\n",
        "from bokeh.palettes import Blues4\n",
        "from bokeh.models import ColumnDataSource,Slider\n",
        "import datetime\n",
        "from bokeh.io import push_notebook\n",
        "from dateutil import parser\n",
        "from ipywidgets import interact, widgets, fixed\n",
        "output_notebook()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2020-07-13T10:54:38.633858Z",
          "end_time": "2020-07-13T10:54:38.683795Z"
        }
      },
      "cell_type": "code",
      "source": [
        "def detect(endpoint, subscription_key, request_data):\n",
        "    headers = {'Content-Type': 'application/json', 'Ocp-Apim-Subscription-Key': cog_key}\n",
        "    response = requests.post(cog_endpoint, data=json.dumps(request_data), headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        return json.loads(response.content.decode(\"utf-8\"))\n",
        "    else:\n",
        "        print(response.status_code)\n",
        "        raise Exception(response.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2020-07-13T11:12:55.242622Z",
          "end_time": "2020-07-13T11:12:55.845752Z"
        }
      },
      "cell_type": "code",
      "source": [
        "def build_figure(sample_data, sensitivity):\n",
        "    sample_data['sensitivity'] = sensitivity\n",
        "    result = detect(endpoint, subscription_key, sample_data)\n",
        "    columns = {'expectedValues': result['expectedValues'], 'isAnomaly': result['isAnomaly'], 'isNegativeAnomaly': result['isNegativeAnomaly'],\n",
        "          'isPositiveAnomaly': result['isPositiveAnomaly'], 'upperMargins': result['upperMargins'], 'lowerMargins': result['lowerMargins'],\n",
        "          'timestamp': [parser.parse(x['timestamp']) for x in sample_data['series']], \n",
        "          'value': [x['value'] for x in sample_data['series']]}\n",
        "    response = pd.DataFrame(data=columns)\n",
        "    values = response['value']\n",
        "    label = response['timestamp']\n",
        "    anomalies = []\n",
        "    anomaly_labels = []\n",
        "    index = 0\n",
        "    anomaly_indexes = []\n",
        "    p = figure(x_axis_type='datetime', title=\"Batch Anomaly Detection ({0} Sensitvity)\".format(sensitivity), width=800, height=600)\n",
        "    for anom in response['isAnomaly']:\n",
        "        if anom == True and (values[index] > response.iloc[index]['expectedValues'] + response.iloc[index]['upperMargins'] or \n",
        "                         values[index] < response.iloc[index]['expectedValues'] - response.iloc[index]['lowerMargins']):\n",
        "            anomalies.append(values[index])\n",
        "            anomaly_labels.append(label[index])\n",
        "            anomaly_indexes.append(index)\n",
        "        index = index+1\n",
        "    upperband = response['expectedValues'] + response['upperMargins']\n",
        "    lowerband = response['expectedValues'] -response['lowerMargins']\n",
        "    band_x = np.append(label, label[::-1])\n",
        "    band_y = np.append(lowerband, upperband[::-1])\n",
        "    boundary = p.patch(band_x, band_y, color=Blues4[2], fill_alpha=0.5, line_width=1, legend='Boundary')\n",
        "    p.line(label, values, legend='Value', color=\"#2222aa\", line_width=1)\n",
        "    p.line(label, response['expectedValues'], legend='ExpectedValue',  line_width=1, line_dash=\"dotdash\", line_color='olivedrab')\n",
        "    anom_source = ColumnDataSource(dict(x=anomaly_labels, y=anomalies))\n",
        "    anoms = p.circle('x', 'y', size=5, color='tomato', source=anom_source)\n",
        "    p.legend.border_line_width = 1\n",
        "    p.legend.background_fill_alpha  = 0.1\n",
        "    show(p, notebook_handle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "## Vizualizing anomalies throughout your data\n",
        "\n",
        "The following cells call the Anomaly Detector API with two different example time series data sets, and different sensitivities for anomaly detection. Varying the sensitivity of the Anomaly Detector API can improve how well the response fits your data."
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "### Example 1: time series with an hourly sampling frequency\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2020-07-13T11:03:56.901284Z",
          "end_time": "2020-07-13T11:04:01.056122Z"
        }
      },
      "cell_type": "code",
      "source": [
        "# Hourly Sample\n",
        "sample_data = json.load(open('sample_hourly.json'))\n",
        "sample_data['granularity'] = 'hourly'\n",
        "sample_data['period'] = 24\n",
        "# 95 sensitivity\n",
        "build_figure(sample_data,95)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "start_time": "2020-07-13T11:04:07.433225Z",
          "end_time": "2020-07-13T11:04:07.679688Z"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "sample_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "start_time": "2020-07-13T11:54:31.450807Z",
          "end_time": "2020-07-13T11:54:44.878985Z"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "daily_data = json.load(open('csvjson.json'))\n",
        "daily_data['granularity'] = 'minutely'\n",
        "\n",
        "# 95 sensitivity\n",
        "build_figure(daily_data,95)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "start_time": "2020-07-13T11:49:46.1108Z",
          "end_time": "2020-07-13T11:49:46.231396Z"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "data['timestamp'] = pd.to_datetime(data['timestamp']).dt.strftime('%Y-%m-%dT%H:%M%:%SZ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "start_time": "2020-07-13T11:50:15.276742Z",
          "end_time": "2020-07-13T11:50:16.190857Z"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "data.to_csv('art_daily_jumpsup2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "start_time": "2020-07-13T11:25:21.903388Z",
          "end_time": "2020-07-13T11:25:25.248263Z"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "series = []\n",
        "data_file = pd.read_csv('art_daily_jumpsup.csv', encoding='utf-8', parse_dates=[0])\n",
        "for index, row in data_file.iterrows():\n",
        "    series.append(Point(timestamp=row[0], value=row[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2020-07-13T10:54:47.959624Z",
          "end_time": "2020-07-13T10:54:51.493215Z"
        }
      },
      "cell_type": "code",
      "source": [
        "# 90 sensitivity\n",
        "build_figure(sample_data,90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "ExecuteTime": {
          "start_time": "2020-07-13T10:54:51.497872Z",
          "end_time": "2020-07-13T10:54:55.388257Z"
        }
      },
      "cell_type": "code",
      "source": [
        "#85 sensitivity\n",
        "build_figure(sample_data,85)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "### Example 2: time series with an daily sampling frequency\n"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "#daily sample\n",
        "sample_data = json.load(open('sample.json'))\n",
        "sample_data['granularity'] = 'daily'\n",
        "# 95 sensitivity\n",
        "build_figure(sample_data,95)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "# 90 sensitivity\n",
        "build_figure(sample_data,90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "# 85 sensitivity\n",
        "build_figure(sample_data,80)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}